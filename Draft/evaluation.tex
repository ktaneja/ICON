\section{Evaluation}
\label{sec:evaluation}

We conducted an evaluation to assess the effectiveness of \tool. In our evaluation, we address two main research questions:

\begin{itemize}
	\item\textbf{RQ1}: What are the precision and recall of \tool\ in identifying temporal constraints from sentences written in natural language?
	\item\textbf{RQ2}: What is the accuracy of \tool\ in inferring temporal constraints from constraint sentences in the API documents?
	\item\textbf{RQ3}: How do the constraints inferred by our approach compare with the typed-enforced temporal constraints?
 
\end{itemize}

\subsection{Subjects}
\label{sub:subject}

We used the API documents of the following two libraries as subjects for our evaluation. 
\begin{itemize}
	\item{Amazon S3}. \amazon\ provides a REST based web services interface that can be used to store and retrieve data on the web. Furthermore, \CodeIn{Amazon S3} also empowers a developer with rich set of API methods to access a highly scalable, reliable, secure, fast, inexpensive infrastructure. \CodeIn{Amazon S3} is reported to store more than 2 trillion objects as of April 2013 and gets over 1.1 million requests per second at peak time~\cite{amazons3stats}.

	\item{java.io}. \CodeIn{java.io} is a popular packacge in \CodeIn{Java} programming language that provides APIs for system input and output through data streams, serialization and the file system.
\end{itemize}
We chose \CodeIn{Amazon S3} and \CodeIn{java.io} APIs as our subjects because they are popular and contain decent documentation.

\subsection{Experimental Setup.} 
We first manually annotated the sentences in the API documents of the two APIs.
Two authors manually labeled each sentence in the API documentation as sentence containing 
temporal constraints or not.
We used \CodeIn{cohen kappa}~\cite{carletta1996assessing} score to statistically measure
the inter-rater agreement.
The \CodeIn{cohen kappa} score of the two authors was .66 (on a scale of 0 to 1), 
which denotes a statically significant agreement. 
After the authors classified all the sentences, they 
discussed with each other to reach a consensus on the sentences they classified differently. 
We use this classified sentences as the golden set for calculating precision and recall.

 


To answer RQ1, we measure the number of true positives ($TP$), false positives ($FP$), true negative ($TN$), and false negatives ($FN$)
in identifying the constraint sentences by \tool.
We define constraint sentence as a sentence describing a temporal constraints.
We define the $TP$, $FP$, $TN$, and $FN$ of \tool\ as follows:

\begin{enumerate}
	\item $TP$: A sentence correctly identified by \tool\ as constraint sentence.
	\item $FP$: A sentence incorrectly identified by \tool\ as constraint sentence.
	\item $TN$: A sentence correctly identified by \tool\ as not a constraint sentence.
	\item $FN$: A sentence incorrectly identified by \tool\ as not a constraint sentence.
\end{enumerate}


In statistical classification~\cite{Olson08}, $Precision$ is defined as a ratio of
number of true positives to the total number of items reported to be true,
$Recall$ is defined as a ratio of number of true positives to the total number
of items that are true. $F-score$ is defined as the weighted harmonic mean of 
$Precision$ and $Recall$. Higher value of $Precision$, $Recall$, and $F-Score$
are indicative of higher quality of the constraint statements inferred using 
\tool. based on the calculation of $TP$, $FP$, $TN$, and $FN$ of \tool\ defined
previously we computed the $Precision$, $Recall$, and $F-Score$ of \tool\ as follows:


\begin{center}

$Precision$ = $\frac{TP}{TP + FP}$

$Recall$ = $\frac{TP}{TP + FN}$

$F-Score$ = $\frac{2 X Precison X Recall}{Precision + Recall}$
\end{center}


To answer RQ2, we checked the temporal constraints inferred from constraint sentences by \tool.
We measure $accuracy$ of \tool as the ratio of the total number of temporal constraints that
are correctly inferred by \tool to the total number of constraint sentences. 

\subsection{Results}

We next describe our evaluation results to demonstrate the effectiveness of \tool\ in identifying temporal constraints.

\begin{table*}
\begin{center}

\caption{Evaluation Results}

\begin{tabular}{lcccccccccccc}
\topline
\headcol API & Mtds & Sen & Sen$_C$ & Sen$_{ICON}$ & TP & FP & FN & P(\%) & R(\%) & F$_S$(\%) & Spec$_{ICON}$ & Acc(\%)\\
\midline 
java.io & 662 & 2417 & 78 & 88 & 57 & 31 & 21 & 64.8 & 73.1 & 68.8 & 56 & 71.8\\ 
\rowcol AMAZON S3 REST & 51 & 1492 & 12 & 12 & 8 & 4 & 4 & 66.7 & 66.7 & 66.7 & 7 & 58.3\\ 
Total & 713 & 3909 & 90 & 100 & 65 & 35 & 25 & 65.0$^*$ & 72.2$^*$ & 68.4$^*$ & 63 & 70.0$^*$\\ 
\bottomlinec
%----------------- END TABLE DATA ------------------------ 
\multicolumn{13}{p{6.5in}}{\small
$^*$ Column average;
Mtds: Total no. of Methods; Sen: Total no. of Sentences; Sen$_C$: Total no. of constraint Sentences;
Sen$_{ICON}$: Total no. of constraint Sentences identified by \tool; 
TP: Total no. of True Positives; FP: Total no. of False Positives; FN: Total no. of False Negatives;
P: Precision; R: Recall; F$_S$: F-Score; Acc: Accuracy
Spec$_{ICON}$: Total no. of temporal constraint correctly identified by \tool;
} \\ 
\end{tabular}
\label{tab:results}
\end{center}
\end{table*}


\subsubsection{RQ1: Effectiveness in Identifying Constraint Sentences}


In this section, we quantify the effectiveness of \tool\ in identifying constraint sentences by answering RQ1.
Table~\ref{tab:results} shows the effectiveness of \tool\ in identifying constraint sentences.
Column ``API'' lists the names of the subject API. 
Columns ``Mtds'' and ``Sen'' lists the number of methods and sentences in each subject API's.
Column ``Sen$_C$'' list the number of manually identified constraint sentences.
Column ``Sen$_{ICON}$'' lists the number of sentences identified by \tool\ as constraint sentences. 
Columns ``TP'', ``FP'', ``TN'', and ``FN'' represent the number of \CodeIn{true positives}, \CodeIn{false positives}, \CodeIn{true negatives}, and \CodeIn{false negatives}, respectively. 
Columns ``P(\%)'', ``R(\%)'', and ``F$_S$(\%)'' list percentage values of \CodeIn{precision}, \CodeIn{recall}, and \CodeIn{F-score} respectively. 
Our results show that, out of 3,909 sentences, \tool\ effectively identifies constraint sentences with the average precision, recall, and F-score of 65.0\%, 72.2\%, and 68.4\%, respectively.

 

%We next present an example to illustrate how \tool\ incorrectly identifies a sentence as a constraint sentence. \textbf{A example goes here}
We next present an example to illustrate how \tool\ incorrectly identifies a sentence as a constraint sentence (producing false positives). For instance, consider the sentence ``\textit{This is done by flushing the stream and then closing the underlying output stream.}'' from  \CodeIn{close} method description from \CodeIn{PrintStream} class. \tool\ incorrectly identifies the action ``flush'' being performed before the action ``close''. However \tool\ fails to make the distinction that it happens internally (enforced within the body) in the method. \tool, thus incorrectly identifies the sentence as a constraint sentence.   


Another major source of FPs is the incorrect parsing of sentences by the underlying NLP infrastructure and/or inadequacy of generic dictionaries for synonym analysis. For instance, consider the sentence ``\textit{If this stream has an associated channel then the channel is closed as well.}'' from the \CodeIn{close} method description from \CodeIn{FileOutputStream}. The sentence describes an effect that happens as a result of calling the \CodeIn{close} method and does not describe any temporal constraint. However, \tool\ annotates the sentence as a constraint sentence because underlying Wordnet dictionaries matches the word ``has'' as a synonym of ``get''. This incorrect matching in turn causes \tool\ to incorrectly annotate the sentence as constraint sentence because ``has'' is matched against \CodeIn(get) method in \CodeIn{FileOutputStream}. We observed 8 instances of previously described example in our results.

If we manually fixed the Wordnet dictionaries to match ``has'' and ``get'' as synonyms, our precision is further increased to 70.8\% effectively increasing the F-Score of \tool\ to 71.2\%. We refrained from including such modifications for reporting the results to stay true to our proposed framework. In the future, we plan to investigate techniques to construct better domain dictionaries for software API.

We next present an example to illustrate how \tool\ fails identify a constraint sentence (producing false negative). False negatives are undesirable in the context of our problem domain, because they can mislead the users of \tool\ into believing that no other temporal constraint exists in the API documents. Furthermore, an overwhelming number of false negatives works against the usefulness of \tool. For instance, consider the sentence ``\textit{This implementation of the PUT operation creates a copy of an object that is already stored in Amazon S3.}'' from  \CodeIn{PUT Object-Copy} method description in \amazonAPI. The sentence describes the constraint that the object must already be stored (invocation of \CodeIn{PUT Object}) before calling the current method. However, \tool cannot make the connection owing to the limitation of the semantic graphs that do not list ``already stored'' as a ``valid operation'' on object.  


Another major source of false negatives (similar to reasons for false positives) is the incorrect parsing of sentences by the underlying NLP infrastructure. For instance, consider the sentence ``\textit{If any in-memory buffering is being done by the application (for example, by a BufferedOutputStream object), those buffers must be flushed into the FileDescriptor (for example, by invoking OutputStream.flush) before that data will be affected by sync.}'' The sentence describes that the \CodeIn{OutputStream.flush()} must be invoked before invoking the current method if in-memory buffering is performed. However, the length and complexity in terms of number of clauses causes the underlying Stanford parser to inaccurately annotate the dependencies, which eventually results into incorrect classification. 

Overall, a significant number of false positives and false negatives will be reduced as the current NLP research advances the underlying NLP infrastructure. Furthermore, use of domain specific dictionaries as opposed to generic dictionaries used in current prototype implementation will further improve the precision and recall of \tool. 

\subsubsection{RQ2: Accuracy in Inferring Temporal Constraints}

In this section, we evaluate the effectiveness of \tool\ in inferring temporal constraints from the identified constraint sentences from API documents. To answer RQ2, we ran the sentences manually identified as constraint sentences against \tool. We then manually verified the correctness of temporal constraints inferred by our \tool. We define the \CodeIn{accuracy} of \tool\ as the ratio of constraint sentences with correctly inferred temporal constraints to the total number of constraint sentences. 

Table~\ref{tab:results} shows the effectiveness of \tool\ in inferring temporal constraints from the identified constraint sentences.
Column ``API'' lists the names of the subject API. 
Columns ``Mtds'' and ``Sen'' list the number of methods and sentences in each subject API's.
Column ``Sen$_C$'' lists the number of manually identified constraint sentences.
Column ``Spec$_{ICON}$'' lists the number of sentences with correctly inferred temporal constraints by \tool. 
Column ``Acc(\%)'' list percentage values of accuracy. 
Our results show that, out of 90 manually identified constraint sentences, \tool\ correctly infers temporal constraints with the average accuracy of 70.0\%.

We next present an example to illustrate how \tool\ incorrectly infers temporal constraints from a constraint sentence. Consider the sentence ``if the stream does not support seek, or if this input stream has been closed by invoking its close method, or an I/O error occurs.'' from \CodeIn{skip} method of \CodeIn{java.io.FilterInputStream} class. Although \tool\ correctly infers that close cannot be called before current method, \tool\ incorrectly associates the phrase ``support seek'' with method \CodeIn{markSupported} in the class. The faulty association happens due to incorrect parsing of the sentence by the underlying NLP infrastructure. Such issues will be alleviated as the underlying NLP infrastructure improves.   

Another, major cause of failure for \tool\ in inferring temporal constraints from sentences is the failure to identify the sentence as a constraint sentences at the first place (false negatives). Overall, accuracy of \tool\ can be significantly improved by lowering the false negative rate in identifying the constraint sentences. 




\subsubsection{RQ3: Comparison to Typed-Enforced Constraints}

In this section, we compared the temporal constraints inferred from the natural language API descriptions to those enforced by the type-system (referred to as type-enforced constraint). The constraints that are enforced by the type-system can be enforced by IDEs. Hence, for such types of constraints, we do not require sophisticated techniques like ICON. 
For \CodeIn{java.io}, we define a type-enforced constraint as a constraint that mandates a method M accepting input parameter I of type T to be invoked after (at least one) a method M0 whose return value is of type T. 
Since there are no types in REST APIs, for \CodeIn{Amazon S3}, we consider a constraint as a type-enforced constraint if it is implicit in the \CodeIn{CRUD} naming convention followed by REST operations. In particular, we consider a constraint as a type-enforced constraint, if the constraint mandates a DELETE, GET, or PUT operation on a resource to be invoked after a GET operation on the same resource. 

To address this question, we manually inspect each of the constraints reported by ICON and classify it as a type-enforced constraint or a non type-enforced constraint. 
We observed that none of the constraints inferred by our \tool\ from natural language text were classified as a type-enforced constraint. Hence, the constraints detected by \tool\ are not trivial enough to be enforced by a type system.




\subsection{Summary}
\label{sub:summary}

In summary, we demonstrate that \tool\ effectively identifies constraint sentences (from over 3900 API sentences) with the average precision, recall, and F-score of 65.0\%, 72.2\%, and 68.4\% respectively. Furthermore, we also show that \tool\ infers temporal constraints from the constraint sentences an average accuracy of 70\%. Furthermore, also provide discussion that a false positives rate and false negatives rate can be further improved by improving the underlying NLP infrastructure. Finally, we provide a comparison of the temporal constraints inferred from natural language description against the temporal constraints enforced by a type system. 


\subsection{Threats to Validity}
\label{sub:threats_to_validity}
Threats to external validity primarily include the degree to which the subject documents used in our evaluations are representative of true practice. To minimize the threat, we used API documents of two different API's: JDK \CodeIn{java.io} and \amazon. On one hand Java is a widely used programming language and \CodeIn{java.io} and is one of the main packages. In contrast, \amazon\ provides HTTP based access to online storage allowing developers the freedom to write clients applications in any programming language. Furthermore, the difference in the functionalities provided by the two API's also address the issue of over fitting our approach to a particular type of API. The threat can be further reduced by evaluating our approach on more subjects. 

Threats to internal validity include the correctness of our implementation in extracting usage constraints and labeling a statement as a constraint statement. To reduce the threat, we manually inspected all the constraints inferred against the API method descriptions in our evaluation. Furthermore, we ensured that the results were individually verified and agreed upon by two authors.




