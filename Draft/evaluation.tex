\section{Evaluation}
\label{sec:evaluation}

We conducted an evaluation to assess the effectiveness of \tool. In our evaluation, we address two main research questions:

\begin{itemize}
	\item\textbf{RQ1}: What are the precision and recall of \tool\  in
	identifying temporal constraints from sentences written in natural language?
	\item\textbf{RQ2}: What is the accuracy of \tool\ in inferring temporal constraints from specification sentences in the API documents? 
\end{itemize}

\subsection{Subjects}
\label{sub:subject}

We used the API documents of the following two libraries as subjects for our evaluation. 
\begin{itemize}
	\item{Amazon S3}. \amazon\ provides a REST based web services interface that can be used to store and retrieve data on the web. Furthermore, \CodeIn{Amazon S3} also empowers a developer with rich set of API methods to access a highly scalable, reliable, secure, fast, inexpensive infrastructure. \CodeIn{Amazon S3} is reported to store more than 2 trillion objects as of April 2013 and gets over 1.1 million requests per second at peak time~\cite{amazons3stats}.

	\item{java.io}. \CodeIn{java.io} is a popular packacge in \CodeIn{Java} programming language that provides APIs for system input and output through data streams, serialization and the file system.
\end{itemize}
We chose \CodeIn{Amazon S3} and \CodeIn{java.io} APIs as our subjects because they are popular and contain decent documentation.

\subsection{Experimental Setup.} 
We first manually annotated the sentences in the API documents of the two APIs.
Two authors manually labeled each sentence in the API documentation as sentence containing 
temporal constraints or not.
We used \CodeIn{cohen kappa}~\cite{carletta1996assessing} score to statistically measure
the inter-rater agreement.
The \CodeIn{cohen kappa} score of the two authors was .66 (on a scale of 0 to 1), 
which denotes a statically significant agreement. 
After the authors classified all the sentences, they 
discussed with each other to reach a consensus on the sentences they classified differently. 
We use this classified sentences as the golden set for calculating precision and recall.

 


To answer RQ1, we measure the number of true positives ($TP$), false positives ($FP$), true negative ($TN$), and false negatives ($FN$)
in identifying the specification sentences by \tool.
We define specification sentence as a sentence describing a temporal constraints.
We define the $TP$, $FP$, $TN$, and $FN$ of \tool\ as follows:

\begin{enumerate}
	\item $TP$: A sentence correctly identified by \tool\ as specification sentence.
	\item $FP$: A sentence incorrectly identified by \tool as specification sentence.
	\item $TN$: A sentence correctly identified by \tool\ as not a specification sentence.
	\item $FN$: A sentence incorrectly identified by \tool\ as not a specification sentence.
\end{enumerate}


In statistical classification~\cite{Olson08}, $Precision$ is defined as a ratio of
number of true positives to the total number of items reported to be true,
$Recall$ is defined as a ratio of number of true positives to the total number
of items that are true. $F-score$ is defined as the weighted harmonic mean of 
$Precision$ and $Recall$. Higher value of $Precision$, $Recall$, and $F-Score$
are indicative of higher quality of the specification statements inferred using 
\tool. based on the calculation of $TP$, $FP$, $TN$, and $FN$ of \tool\ defined
previously we computed the $Precision$, $Recall$, and $F-Score$ of \tool\ as follows:


\begin{center}

$Precision$ = $\frac{TP}{TP + FP}$

$Recall$ = $\frac{TP}{TP + FN}$

$F-Score$ = $\frac{2 X Precison X Recall}{Precision + Recall}$

\end{center}

To answer RQ2, we checked the temporal constraints inferred from specification sentences by \tool.
We measure $accuracy$ of \tool as the ratio of the total number of temporal constraints that
are correctly inferred by \tool to the total number of specification sentences. 

\subsection{Results}

We next describe our evaluation results to demonstrate the effectiveness of \tool\ in identifying temporal constraints.

\subsubsection{RQ1: Effectiveness in Identifying Specification Sentences}


In this section, we quantify the effectiveness of \tool\ in identifying specification sentences by answering RQ1. Our results show that, out of 2,414 sentences, \tool\ effectively identifies specification sentences with the average precision, recall, F-score, and accuracy of 65.0\%, 71.8\%, 68.7\%, and 97.9\%, respectively.

We next present an example to illustrate how \tool\ incorrectly identifies a sentence as a specification sentence. \textbf{A example goes here}

We next present an example to illustrate how \tool\ incorrectly identifies a sentence as a specification sentence (producing false positives). For instance, consider the sentence ``\textit{This is done by flushing the stream and then closing the underlying output stream.}'' from  \CodeIn{close} method description from \CodeIn{PrintStream} class. \tool\ incorrectly identifies the action ``flush'' being performed before the action ``close''. However \tool\ fails to make the distinction that it happens internally (enforced within the body) in the method. \tool, thus incorrectly identifies the sentence as a specification sentence.   


Another major source of FPs is the incorrect parsing of sentences by the underlying NLP infrastructure and/or inadequacy of generic dictionaries for synonym analysis. For instance, consider the sentence ``\textit{If this stream has an associated channel then the channel is closed as well.}'' from the \CodeIn{close} method description from \CodeIn{FileOutputStream}. The sentence describes an effect that happens as a result of calling the \CodeIn{close} method and does not describe any temporal constraint. However, \tool\ annotates the sentence as a specification sentence because underlying Wordnet dictionaries matches the word ``has'' as a synonym of ``get''. This incorrect matching in turn causes \tool to incorrectly annotate the sentence as specification sentence because ``has'' is matched against \CodeIn(get) method in \CodeIn{FileOutputStream}. We observed 8 instances of previously described example in our results.

If we manually fixed the Wordnet dictionaries to match ``has'' and ``get'' as synonyms, our precision is further increased to 70.8\% effectively increasing the F-Score of \tool to 71.2\%. We refrained from including such modifications for reporting the results to stay true to our proposed framework. In the future, we plan to investigate techniques to construct better domain dictionaries for software API.

We next present an example to illustrate how \tool\ fails identify a specification sentence (producing false negative). False negatives are particularly undesirable in the context of our problem domain, because they can mislead the users of \tool\ into believing that no other temporal constraint exists in the API document. Furthermore, an overwhelming number of false negatives works against the usefulness of \tool. For instance, consider the sentence ``\textit{This is done by flushing the stream and then closing the underlying output stream.}'' from  \CodeIn{close} method description from \CodeIn{PrintStream} class. The sentence describes the constraint in terms of invoking the same method again. Although \tool\ correctly identifies the method \CodeIn{close} in the sentence, \tool\ currently does not support temporal constraint on the same method. Aforementioned, limitation causes \tool\ to not identify the statement as a specification constraint.   


Another major source of false negatives (similar to reasons for false positives) is the incorrect parsing of sentences by the underlying NLP infrastructure. For instance, consider the sentence ``\textit{If any in-memory buffering is being done by the application (for example, by a BufferedOutputStream object), those buffers must be flushed into the FileDescriptor (for example, by invoking OutputStream.flush) before that data will be affected by sync.}'' The sentence describes that the \CodeIn{OutputStream.flush()} must be invoked before invoking the current method if in-memory buffering is performed. However, the length and complexity in terms of number of clauses causes the underlying Stanford parser to inaccurately annotate the dependencies, which eventually results into incorrect classification. 

Overall, a significant number of false positives and false negatives will be reduced as the current NLP research advances the underlying NLP infrastructure. Furthermore, use of domain specific dictionaries as opposed to generic dictionaries used in current prototype implementation will further improve the precision and recall of \tool. 

\subsubsection{RQ2: Accuracy in Inferring Temporal Constraints}

In this section, we evaluate the effectiveness of \tool\ in inferring specification sentences from API documents.

\subsection{Summary}
\label{sub:summary}



\subsection{Threats to Validity}
\label{sub:threats_to_validity}
Threats to external validity primarily include the degree to which the subject documents used in our evaluations are representative of true practice. To minimize the threat, we used API documents of two representative commercial REST API: one dealing with online storage and the other \textbf{TBD}. The \amazon documents describe one of the most popularly used and online storage APIs. We also used the \textbf{TBD}. Furthermore, the difference in the functionalities provided by the two projects also address the issue of over fitting our approach to a particular type of API. The threat can be further reduced by evaluating our approach on more subjects. 

Threats to internal validity include the correctness of our implementation in extracting usage constraints and labelling a statement as a constraint statement. To reduce the threat, we manually inspected all the constraints inferred against the API method descriptions in our evaluation. Furthermore, we ensured that the results were individually verified and agreed
upon by two authors.




If any in-memory buffering is being done by the application (for example, by a BufferedOutputStream object), those buffers must be flushed into the FileDescriptor (for example, by invoking OutputStream.flush) before that data will be affected by sync.


If markposMM is -1MM (no mark has been set or the mark has been invalidated), an IOExceptionMM is thrown.