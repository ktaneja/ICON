\section{Background}
\label{sec:background}

We next briefly introduce the techniques used in this work that have been grouped into two broad categories.
We first introduce the core NLP techniques used in this work.
We then introduce the SE specific NLP techniques proposed in related work~\cite{pandita12:inferring,pandita13:WHYPER} that are leveraged in this work.

\subsection{Core NLP techniques}
\label{sub:CoreNLPback}

%Recently, a lot of exciting work has been carried out in the area of Natural Language Processing (NLP), with existing NLP techniques proving to be fairly accurate in highlighting grammatical structure of a natural language sentence. However, existing NLP techniques are still in the processing phase and not in understanding phase.  We briefly introduce the NLP techniques used in this work.


{\small $\bullet$} \textbf{Parts Of Speech (POS) tagging}~\cite{Klein03,KleinNIPS03}: Also known as \textit{`word tagging'} and \textit{`grammatical tagging'}, these techniques aim to identify the part of speech (such as noun, verbs, etc.), a particular word in a sentence belongs to.

{\small $\bullet$} \textbf{Phrase and Clause Parsing}: Also known as chunking, this technique divides a sentence into a constituent set of words (or phrases) that logically belong together (such as a Noun Phrase and Verb Phrase). Chunking further enhances the syntax of a sentence in addition to POS tagging.

{\small $\bullet$} \textbf{Typed Dependencies}~\cite{Marneffe06LREC,Marneffe08COLING}: The Stanford typed dependencies representation  is designed to provide a simple description of grammatical relationships directed towards non-linguistics experts to perform NLP related tasks. The representation provides a hierarchical structure for the dependencies with precise definitions of what each dependency represents, thus facilitating machine based manipulation of natural language text.

%\textbf{Named Entity Recognition}~\cite{Finkel05ACL}. Also known as \textit{`entity identification'} and \textit{`entity extraction'}, these techniques are a subtask of information extraction that aims to classify words in a sentence into predefined categories such as names, quantities, expression of times, etc. These techniques help in associating predefined semantic meaning to a word or a group of words (phrase), thus facilitating semantic processing of named entities. 

%\textbf{Co-reference Resolution}~\cite{RaghunathanEMNLP10,LeeCoNLL11}. Also known as \textit{`anaphora resolution'}, these techniques aim to identify multiple expressions present across (or within) the sentences, that point out to the same thing or \textit{`referant'}. These techniques are useful for extracting information; especially if the information encompasses many sentences in a document.

%We next present SE specific NLP techniques.

\subsection{SE specific NLP techniques}
\label{sub:SENLPback}

{\small $\bullet$} \textbf{Programming Keywords}~\cite{pandita12:inferring}: Accurate annotation of POS tags in a sentence is fundamental to effectiveness of any advanced NLP technique.
However POS tagging developed for well written news articles, often performs unsatisfactorily on domain-specific text.
In particular, with respect to API documents certain words have a different semantic meaning, in contrast to general linguistics that causes incorrect annotation of POS tags.

For instance, consider the word \CodeIn{POST}. 
The online Oxford dictionary %\footnote{\url{http://oxforddictionaries.com/us/definition/american_english/post?q=POST}}
has eight different definition of word \CodeIn{POST}, and none of them describes \CodeIn{POST} as an HTTP method\footnote{In HTTP vocabulary \CodeIn{POST} means: \textit{``Creates a new entry in the collection.
The new entry's URI is assigned automatically and is usually returned by the operation''}}
supported by REST API.
Thus existing POS tagging techniques fail to accurately annotate the POS tags of the sentences involving word \CodeIn{POST}.  

Noun boosting identifies such words from the sentences based on a domain-specific dictionaries, and annotates them appropriately.
The annotation directs the POS tagger to correctly annotate the POS tags of the domain specific words thus increasing accuracy of subsequent NLP techniques such as chunking and typed dependency annotation.

{\small $\bullet$} \textbf{Lexical Token Reduction}~\cite{pandita13:WHYPER}:
The accuracy of core NLP techniques is often inversely proportional to the number of lexical tokens in a sentence.
Thus, reduction in the number of lexical tokens greatly improves the accuracy of core NLP techniques. 
In particular, following heuristics have been used in our previous work~\cite{pandita12:inferring,pandita13:WHYPER} to achieve the desired reduction of lexical tokens:
%These are a collection of generic preprocessing heuristics to further improve the accuracy of core NLP techniques.

%\item \textit{Period Handling}: Besides marking the end of a sentence in generic English, the character period (`.') has other legal usages as well, such as decimal representation (periods between numbers).
%Although legal, such usage hinder detection of sentence boundaries, thus causing core NLP techniques to return incorrect or imprecise results.
%The text is pre-processed by annotating these usages for accurate detection of sentence boundaries.

\begin{itemize}

\item \textit{Named Entity Handling}: Sometimes a sequence of words correspond to the name of entities that have a specific meaning collectively.
For instance, consider the phrases \textit{``Amazon S3'', ``Amazon simple storage service''}, which are the names of the service.
Further resolution of such phrases using grammatical syntax is redundant for inference needs.
This heuristic annotates the phrase representing the name of an entity as a single lexical token,
to improve accuracy of core NLP techniques.
	
\item \textit{Abbreviation Handling}: Natural-language sentences often consist of abbreviations interleaved with text.
This interleaving may result in POS tagger to incorrectly parse a sentence.
This heuristic finds such instances and annotates them as a single lexical unit.
For example, text followed by abbreviations such as \textit{``Access Control Lists (ACL)''} is treated as single lexical unit.
Detecting such abbreviations is achieved by using the common structure of abbreviations and encoding such structures into regular expressions.
Typically, regular expressions provide a reasonable approximation for handling abbreviations.  

\end{itemize}

{\small $\bullet$} \textbf{Intermediate-Representation Generation}~\cite{pandita13:WHYPER}:
This technique accepts the syntax-annotated sentences (grammatical and semantic) and builds a First-Order-Logic (FOL) like representation of the sentence.
Earlier researches have shown the adequacy using FOL for NLP related analysis tasks~\cite{Sinha2009,Sinha2010,pandita12:inferring}.
Particularly, WHYPER~\cite{pandita13:WHYPER} demonstrates the effectiveness of this representation, by constructing an intermediate representation generator from semantically annotated sentences. 
The component is implemented as a sequence of cascading finite state machines based on the function of annotated Stanford-typed dependencies~\cite{Marneffe06LREC,Marneffe08COLING,Klein03,KleinNIPS03}.
